# Lesson 1: Attention Mechanism

**The breakthrough that started the AI revolution!**

---

## üéØ What You'll Learn

- ‚úÖ Why we need attention (the problem)
- ‚úÖ How attention works (the solution)
- ‚úÖ Query, Key, Value (Q, K, V) concept
- ‚úÖ Attention score calculation
- ‚úÖ Implementation from scratch

**Time:** 2-3 hours
**Difficulty:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ

---

## ü§î The Problem: Why Do We Need Attention?

### What's Wrong with What We've Learned?

**From Projects 1-3, we used bag-of-words:**

```python
sentence1 = "The cat sat on the mat"
sentence2 = "The mat sat on the cat"

# Both become same vector:
[1, 1, 1, 2, 1]  # [cat, mat, on, sat, the]
  cat mat on sat the

# Problem: SAME representation, DIFFERENT meaning!
```

**We lost word order and relationships!**

---

### Real-World Example

**Spam Detection (Project 1):**
```
"This is not a bad movie"  ‚Üí Negative (WRONG!)
                  ‚Üë
            "not" + "bad" should = positive!
```

**Sentiment Analysis (Project 3):**
```
"The food was good but the service was terrible"
                ‚Üë                        ‚Üë
           positive                  negative

# Which matters more? Depends on context!
```

**Bag-of-words can't handle this!**

---

## üí° The Solution: Attention Mechanism

### The Core Idea

**Attention lets the model "focus" on relevant parts of the input.**

**Analogy 1: Reading a Book**
```
Question: "What color is the cat?"

Text: "The brown dog and the black cat sat together."
                             ‚Üë‚Üë‚Üë‚Üë‚Üë
                    Your attention focuses here!

Answer: "black"
```

**Your brain doesn't treat all words equally - it focuses on relevant ones!**

---

**Analogy 2: Search Engine**

```
Your Query: "best pizza restaurant"

Database:
- Restaurant A: "Best burgers in town"      (Score: 0.3)
- Restaurant B: "Amazing pizza, great taste" (Score: 0.9) ‚Üê Focus!
- Restaurant C: "Coffee shop with pastries"  (Score: 0.1)

Attention ‚Üí Focus on B (highest score)
```

**Attention mechanism works exactly like this!**

---

## üîë Query, Key, Value (Q, K, V)

### The Three Components

Think of attention like a **search/retrieval system**:

| Component | Database Analogy | What It Does |
|-----------|------------------|--------------|
| **Query (Q)** | Your search term | "What am I looking for?" |
| **Key (K)** | Database index | "How well do I match?" |
| **Value (V)** | Actual data | "Information to return" |

---

### Concrete Example

**Database of Students:**

```python
# Keys (student IDs - how we search)
keys = ["john_123", "mary_456", "bob_789"]

# Values (student info - what we retrieve)
values = [
    "John, Age 20, CS major",
    "Mary, Age 22, Math major",
    "Bob, Age 21, CS major"
]

# Query (what we're looking for)
query = "CS major students"

# Attention process:
# 1. Compare query with each key
scores = [
    similarity(query, "john_123") = 0.8,  # High (CS major)
    similarity(query, "mary_456") = 0.2,  # Low (Math major)
    similarity(query, "bob_789") = 0.9    # High (CS major)
]

# 2. Convert scores to probabilities (softmax)
weights = softmax(scores) = [0.35, 0.05, 0.60]

# 3. Weighted combination of values
output = 0.35 * "John..." + 0.05 * "Mary..." + 0.60 * "Bob..."
       = Mostly Bob and John (CS majors!)
```

---

### In Neural Networks

**Same concept, different implementation:**

```python
# Input: word embeddings
sentence = "The cat sat on the mat"

# For simplicity, let's focus on one word: "sat"
query = embedding["sat"]  # What "sat" is looking for

# All words in sentence
keys = [
    embedding["The"],   # How well does this match?
    embedding["cat"],   # How well does this match?
    embedding["sat"],   # How well does this match?
    embedding["on"],    # How well does this match?
    embedding["the"],   # How well does this match?
    embedding["mat"]    # How well does this match?
]

values = keys  # Same as keys in self-attention

# Calculate attention:
scores = query @ keys.T  # Dot product = similarity
weights = softmax(scores)  # Normalize to probabilities
output = weights @ values  # Weighted average
```

---

## üßÆ The Math (Simple Version)

### Step-by-Step Calculation

**Given:**
- **Query (Q)**: What we're looking for (shape: d)
- **Keys (K)**: What we're comparing against (shape: n √ó d)
- **Values (V)**: What we want to retrieve (shape: n √ó d)

Where:
- n = number of elements (e.g., words in sentence)
- d = embedding dimension (e.g., 512)

---

**Step 1: Compute Similarity Scores**
```python
scores = Q @ K.T  # Matrix multiplication

# Shape: (d,) @ (d, n) = (n,)
# Result: One score per key
```

**What's happening:**
- Dot product measures similarity
- High score = query and key are similar
- Low score = query and key are different

---

**Step 2: Scale the Scores**
```python
scores = scores / sqrt(d_k)

# d_k = dimension of keys (usually same as d)
```

**Why scale?**
- Dot products grow with dimension
- Scaling keeps gradients stable
- Mathematical detail (can skip for now!)

---

**Step 3: Apply Softmax (Get Probabilities)**
```python
attention_weights = softmax(scores)

# softmax(x_i) = exp(x_i) / sum(exp(x_j))
```

**What this does:**
- Converts scores to probabilities
- All weights sum to 1.0
- Higher scores get higher weights

**Example:**
```python
scores = [2.0, 1.0, 3.0]
weights = softmax(scores) = [0.24, 0.09, 0.67]
                              ‚Üë           ‚Üë
                           low         high!
```

---

**Step 4: Weighted Sum of Values**
```python
output = attention_weights @ V

# Shape: (n,) @ (n, d) = (d,)
# Result: Weighted combination of all values
```

**What this means:**
- Take weighted average of values
- Values with high attention get more weight
- Values with low attention contribute less

---

### Complete Formula

**The famous attention formula:**

```
Attention(Q, K, V) = softmax(Q @ K.T / sqrt(d_k)) @ V
```

**That's it!** This simple formula powers ChatGPT!

---

## üìä Visual Example

### Example Sentence: "The cat sat"

**Setup:**
```python
# Word embeddings (simplified to 2D for visualization)
embeddings = {
    "The": [1.0, 0.1],
    "cat": [0.2, 0.9],
    "sat": [0.8, 0.3]
}

# Let's compute attention for "sat"
query = [0.8, 0.3]  # "sat" embedding
```

---

**Step 1: Compute Scores**
```python
keys = [
    [1.0, 0.1],  # "The"
    [0.2, 0.9],  # "cat"
    [0.8, 0.3]   # "sat"
]

# Dot product with query
scores = [
    0.8*1.0 + 0.3*0.1 = 0.83,  # "The"
    0.8*0.2 + 0.3*0.9 = 0.43,  # "cat"
    0.8*0.8 + 0.3*0.3 = 0.73   # "sat"
]
```

---

**Step 2: Apply Softmax**
```python
weights = softmax([0.83, 0.43, 0.73])
        = [0.41, 0.18, 0.41]

# Interpretation:
# "sat" pays 41% attention to "The"
# "sat" pays 18% attention to "cat"
# "sat" pays 41% attention to "sat" (itself!)
```

---

**Step 3: Weighted Sum**
```python
output = 0.41 * [1.0, 0.1] +   # "The"
         0.18 * [0.2, 0.9] +   # "cat"
         0.41 * [0.8, 0.3]     # "sat"

       = [0.784, 0.286]

# New representation of "sat" that incorporates context!
```

---

## üé® Visualization

**Attention Weights Matrix:**

```
         Attending to:
         The   cat   sat
Query:
The     [0.6   0.2   0.2]
cat     [0.3   0.5   0.2]
sat     [0.4   0.2   0.4]

Interpretation:
- "The" mostly attends to itself (0.6)
- "cat" mostly attends to itself (0.5)
- "sat" splits attention between "The" and itself
```

**Heatmap visualization:**
```
        The   cat   sat
The     ‚ñà‚ñà‚ñà   ‚ñà‚ñà    ‚ñà‚ñà
cat     ‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà
sat     ‚ñà‚ñà‚ñà   ‚ñà‚ñà    ‚ñà‚ñà‚ñà

Darker = more attention
```

---

## üíª Implementation from Scratch

### Simple NumPy Implementation

```python
import numpy as np

def softmax(x):
    """Numerically stable softmax."""
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

def attention(Q, K, V):
    """
    Compute attention mechanism.

    Args:
        Q: Query vector (d,) or matrix (m, d)
        K: Key matrix (n, d)
        V: Value matrix (n, d)

    Returns:
        output: Attention output (d,) or (m, d)
        weights: Attention weights (n,) or (m, n)
    """
    # Step 1: Compute scores (similarity)
    scores = Q @ K.T  # (m, d) @ (d, n) = (m, n)

    # Step 2: Scale
    d_k = K.shape[-1]
    scores = scores / np.sqrt(d_k)

    # Step 3: Softmax (normalize to probabilities)
    if scores.ndim == 1:
        weights = softmax(scores)
    else:
        weights = np.array([softmax(row) for row in scores])

    # Step 4: Weighted sum of values
    output = weights @ V  # (m, n) @ (n, d) = (m, d)

    return output, weights
```

---

### Example Usage

```python
# Example: 3 words, 4-dimensional embeddings
sentence = ["The", "cat", "sat"]

# Word embeddings (randomly initialized for demo)
embeddings = np.random.randn(3, 4)

K = embeddings  # Keys
V = embeddings  # Values
Q = embeddings  # Queries

# Compute attention
output, weights = attention(Q, K, V)

print("Input shape:", embeddings.shape)   # (3, 4)
print("Output shape:", output.shape)      # (3, 4)
print("Weights shape:", weights.shape)    # (3, 3)

print("\nAttention weights:")
print(weights)
# Shows which words attend to which!
```

---

## üîç Understanding Attention Weights

### What Do They Mean?

**Example output:**
```python
Attention weights:
       The   cat   sat
The   [0.5   0.3   0.2]
cat   [0.2   0.6   0.2]
sat   [0.3   0.2   0.5]
```

**Interpretation:**

**Row 1 (The):**
- "The" pays 50% attention to itself
- "The" pays 30% attention to "cat"
- "The" pays 20% attention to "sat"

**Row 2 (cat):**
- "cat" pays 60% attention to itself (most important!)
- Pays less attention to "The" and "sat"

**Row 3 (sat):**
- "sat" pays 50% attention to itself
- Also considers "The" (30%)

---

### Why This Is Powerful

**Traditional bag-of-words:**
```python
"The cat sat" ‚Üí [the, cat, sat]
# No relationship between words!
```

**With attention:**
```python
"The cat sat" ‚Üí Context-aware representations
# Each word knows about related words!
# "cat" knows it's connected to "sat" (subject-verb)
```

---

## üéØ Connection to Transformers

### Where Does Attention Fit?

**In a transformer:**
1. **Words ‚Üí Embeddings** (you know this!)
2. **Embeddings ‚Üí Q, K, V** (linear transformations)
3. **Attention(Q, K, V)** ‚Üê This lesson!
4. **Feed-forward network** (Module 3 knowledge!)
5. **Repeat many times** (deep network)

**You just learned the core of transformers!**

---

## üîë Key Takeaways

### Remember These Points

1. **Attention = Weighted average**
   - Not magic, just weighted sum!

2. **Q, K, V analogy = Search engine**
   - Query: what you're looking for
   - Keys: how to find it
   - Values: what to return

3. **Dot product = Similarity**
   - High dot product = similar vectors
   - Measures relevance

4. **Softmax = Probabilities**
   - Converts scores to weights
   - All weights sum to 1.0

5. **Output = Context-aware representation**
   - Each word knows about related words
   - Captures relationships!

---

## üß™ Practice Problems

### Problem 1: Manual Calculation

Given:
```python
Q = [1, 2]
K = [[1, 0], [0, 1], [1, 1]]
V = [[2, 3], [4, 5], [6, 7]]
```

Calculate:
1. Scores (Q @ K.T)
2. Softmax weights
3. Output (weights @ V)

<details>
<summary>Solution</summary>

```python
# Step 1: Scores
scores = Q @ K.T = [1, 2] @ [[1, 0], [0, 1], [1, 1]].T
       = [1*1 + 2*0, 1*0 + 2*1, 1*1 + 2*1]
       = [1, 2, 3]

# Step 2: Softmax (ignoring scaling)
weights = softmax([1, 2, 3])
        = [e^1/(e^1+e^2+e^3), e^2/(e^1+e^2+e^3), e^3/(e^1+e^2+e^3)]
        ‚âà [0.09, 0.24, 0.67]

# Step 3: Output
output = weights @ V
       = 0.09*[2,3] + 0.24*[4,5] + 0.67*[6,7]
       ‚âà [5.2, 6.2]
```
</details>

---

### Problem 2: Implement Scaled Dot-Product

Implement attention with scaling:

```python
def scaled_attention(Q, K, V):
    # Your code here
    pass

# Test
Q = np.array([1.0, 2.0])
K = np.array([[1, 0], [0, 1], [1, 1]])
V = np.array([[2, 3], [4, 5], [6, 7]])

output, weights = scaled_attention(Q, K, V)
print("Output:", output)
print("Weights:", weights)
```

<details>
<summary>Solution</summary>

```python
def scaled_attention(Q, K, V):
    d_k = K.shape[-1]
    scores = Q @ K.T / np.sqrt(d_k)
    weights = softmax(scores)
    output = weights @ V
    return output, weights
```
</details>

---

## üåü Real-World Application

### Machine Translation Example

**English ‚Üí French: "The cat sat"**

**Without attention:**
- Process sequentially
- Loses context
- Poor translation

**With attention:**
```python
Translating "sat":
- Looks at "The" (0.2) - article, less important
- Looks at "cat" (0.7) - subject, very important!
- Looks at "sat" (0.1) - itself

Output: "s'est assis" (correctly agrees with "cat" in French!)
```

**Attention helps the model know that "sat" should agree with "cat"!**

---

## üìñ Further Reading

### Next Steps

1. **Run example code:**
   - `examples/example_01_attention.py`
   - See attention in action!

2. **Next lesson:**
   - `02_self_attention.md`
   - Same mechanism, but Q=K=V!

3. **Deep dive:**
   - "Attention Is All You Need" paper (Section 3.2)
   - Bahdanau attention (original 2014 paper)

---

## ‚úÖ Self-Check

Before moving to Lesson 2, ensure you can:

- [ ] Explain attention mechanism to someone
- [ ] Describe Q, K, V roles
- [ ] Calculate attention by hand (simple example)
- [ ] Implement attention in NumPy
- [ ] Understand why attention is useful

**If you checked all boxes:** Ready for Lesson 2! üéâ

**If not:** Review this lesson, run examples, ask questions!

---

## üí¨ Common Questions

**Q: Why use dot product for similarity?**
A: Fast to compute (matrix multiplication) and works well in practice. High-dimensional vectors with similar directions have high dot products.

**Q: Why divide by sqrt(d_k)?**
A: Prevents dot products from getting too large as dimensions increase. Keeps gradients stable during training.

**Q: What if all weights are equal?**
A: Then attention is useless - just averaging! Good attention should be selective (focus on important parts).

**Q: Is this the same as human attention?**
A: Similar idea (focus on relevant info) but different implementation. This is math, not neuroscience!

---

## üéä Congratulations!

**You've learned the core innovation that powers ChatGPT!**

Everything else in transformers builds on this foundation.

**Key achievement:**
> You understand attention mechanism - the breakthrough that started the AI revolution!

---

**Next:** Lesson 2 - Self-Attention (Q = K = V!)

**Time to next lesson:** When you're comfortable with this material

**Don't rush!** Master this before moving on. üöÄ
