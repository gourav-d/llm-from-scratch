# Getting Started: Sentiment Analysis

**Step-by-step guide to building a movie review classifier**

---

## ğŸ“‹ Prerequisites

âœ… **Module 2: NumPy**
âœ… **Module 3: Neural Networks (all 6 lessons)**
âœ… **Project 1: Email Spam Classifier** (text preprocessing)
âœ… **Project 2: MNIST Digits** (multi-layer networks)

**Time Required:** 3-4 hours

---

## ğŸ¯ Learning Goals

By the end of this project:

1. âœ… Understand sentiment analysis (positive vs negative)
2. âœ… Learn word embeddings (better than bag-of-words!)
3. âœ… Work with longer text sequences
4. âœ… Bridge from simple NLP to transformers
5. âœ… Achieve 85-88% accuracy on movie reviews

---

## ğŸ¬ What is Sentiment Analysis?

**Goal:** Determine if text expresses positive or negative sentiment

**Examples:**
```
"This movie was absolutely brilliant!" â†’ POSITIVE âœ“
"Waste of time and money" â†’ NEGATIVE âœ“
"It was okay, not great" â†’ NEGATIVE (mixed)
```

**Real-World Uses:**
- Product reviews (Amazon, Yelp)
- Social media monitoring (Twitter sentiment)
- Customer feedback
- Brand reputation
- Movie recommendations

---

## ğŸ“Š Dataset

**IMDB Movie Reviews:**
- 50,000 reviews (25k train, 25k test)
- Binary labels: positive or negative
- Average length: ~200-300 words
- Real user reviews from imdb.com

---

## ğŸš¦ Quick Start

### Step 1: Run Simple Version (bag-of-words)

```bash
cd projects/neural_networks/sentiment_analysis
python project_simple.py
```

**Expected output:**
```
Step 1: Loading IMDB reviews...
âœ“ 25,000 training reviews
âœ“ 25,000 test reviews
âœ“ Average length: 234 words

Step 2: Building vocabulary...
âœ“ Vocabulary size: 5,000 words

Step 3: Creating bag-of-words features...
âœ“ Feature matrix: (25000, 5000)

Step 4: Training neural network...
Epoch 10/30: Loss=0.312, Acc=86.2%, Test Acc=83.5%
Epoch 20/30: Loss=0.245, Acc=89.1%, Test Acc=85.2%
Epoch 30/30: Loss=0.198, Acc=91.3%, Test Acc=85.8%

Step 5: Testing...
âœ“ Test Accuracy: 85.8%

Done!
```

---

## ğŸ”‘ Key Concepts

### 1. Sentiment vs Spam Classification

| Aspect | Spam (Project 1) | Sentiment (Project 3) |
|--------|------------------|----------------------|
| **Challenge** | Easier (clear patterns) | Harder (context matters) |
| **Key words** | "free", "buy", "win" | "amazing", "terrible", "boring" |
| **Negation** | Less important | Critical! ("not good") |
| **Context** | Word presence enough | Word order matters |
| **Accuracy** | 93-95% | 85-88% |

### 2. Word Embeddings

**Bag-of-words limitation:**
```
"good" â†’ [0,0,1,0,0,0,...]
"great" â†’ [0,0,0,1,0,0,...]
"excellent" â†’ [0,0,0,0,1,0,...]

Problem: No relationship between similar words!
```

**Word embeddings solution:**
```
"good" â†’ [0.7, 0.3, -0.2, ...]      â†˜
"great" â†’ [0.72, 0.28, -0.18, ...]   â†’ Similar vectors!
"excellent" â†’ [0.75, 0.25, -0.22, ...]â†—

"bad" â†’ [-0.6, -0.4, 0.3, ...]       â†’ Different vector!
```

**Benefits:**
- Similar words get similar vectors
- Learns from data
- Captures semantic meaning
- Better generalization

---

## ğŸ“ˆ Architecture Comparison

### Simple Version (Bag-of-Words)
```
Input (5000) â†’ Hidden (64) â†’ Output (1)
 [word counts]    [ReLU]      [Sigmoid]

Fast but limited - ignores word order
```

### Advanced Version (Word Embeddings)
```
Input (sequence of word IDs)
    â†“
Embedding Layer (learns 100D vectors)
    â†“
Hidden (64, ReLU)
    â†“
Output (1, Sigmoid)

Slower but better - learns word meanings!
```

---

## ğŸ“ What You'll Learn

### From Project Simple
âœ… Sentiment classification basics
âœ… Longer text handling
âœ… Challenging test case (negation, sarcasm)
âœ… Baseline performance with bag-of-words

### From Project Main (Advanced)
âœ… Word embedding layer
âœ… Learning word representations
âœ… Average pooling over sequences
âœ… Better handling of word order
âœ… Bridge to transformers!

---

## ğŸ” Understanding Failures

**Where simple bag-of-words fails:**

```
Review: "This movie was not good at all"
Bag-of-words sees: "good" â†’ POSITIVE prediction âœ—
Should be: NEGATIVE

Review: "I expected it to be terrible, but it was amazing!"
Bag-of-words sees: "terrible" â†’ NEGATIVE prediction âœ—
Should be: POSITIVE
```

**Word embeddings help but not perfect:**
- Still struggles with complex negation
- Doesn't fully understand word order
- â†’ Need attention mechanism (Module 4!)

---

## ğŸ’¡ Connection to Transformers

This project is the **bridge to Module 4**!

**What you have now:**
- âœ… Text â†’ numbers (tokenization)
- âœ… Word embeddings
- âœ… Neural network classification

**What transformers add (Module 4):**
- ğŸ”® **Attention mechanism** - focus on relevant words
- ğŸ”® **Positional encoding** - understand word order
- ğŸ”® **Self-attention** - words relate to each other
- ğŸ”® **Complete context** - understand full meaning

**After Module 4:**
You'll build a mini-GPT that understands context 10x better!

---

## â­ï¸ Next Steps After This Project

**Option 1: Move to Module 4 (Recommended!)**
You're ready for transformers!
- Attention mechanism
- GPT architecture
- Build mini-LLM

**Option 2: Improve Sentiment**
- Try GloVe embeddings
- Implement LSTM (recurrent)
- Try different architectures

**Option 3: Custom Application**
- Collect your own reviews
- Product review classifier
- Social media sentiment

---

## âœ… Success Criteria

Complete this project when you can:

âœ… Explain word embeddings vs bag-of-words
âœ… Understand why sentiment is harder than spam
âœ… Train both simple and advanced versions
âœ… Achieve 85%+ test accuracy
âœ… Understand where the model fails
âœ… Ready for transformers (Module 4!)

---

**Let's build a sentiment classifier!** ğŸš€

ğŸ‘‰ **Next:** Run `python project_simple.py`
